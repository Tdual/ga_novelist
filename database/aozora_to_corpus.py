#!/usr/bin/env python3
"""
é’ç©ºæ–‡åº«ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦RDS PostgreSQLã«æŠ•å…¥
"""
import re
import json
import uuid
import psycopg2
import requests
import zipfile
import io
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

# é’ç©ºæ–‡åº«ã®ä½œå“æƒ…å ±
# ä½œå“ç•ªå·: (ã‚¿ã‚¤ãƒˆãƒ«, ä½œè€…, ã‚¸ãƒ£ãƒ³ãƒ«)
AOZORA_WORKS = {
    # horror
    '482': ('äººé–“æ¤…å­', 'æ±Ÿæˆ¸å·ä¹±æ­©', 'horror'),
    '427': ('èŠ è™«', 'æ±Ÿæˆ¸å·ä¹±æ­¥', 'horror'),
    '235': ('ãƒ‰ã‚°ãƒ©ãƒ»ãƒã‚°ãƒ©', 'å¤¢é‡ä¹…ä½œ', 'horror'),
    '4308': ('ç“¶è©°åœ°ç„', 'å¤¢é‡ä¹…ä½œ', 'horror'),
    
    # romance  
    '756': ('ã¿ã ã‚Œé«ª', 'ä¸è¬é‡æ™¶å­', 'romance'),
    '1569': ('ãŸã‘ãã‚‰ã¹', 'æ¨‹å£ä¸€è‘‰', 'romance'),
    '1147': ('ã«ã”ã‚Šãˆ', 'æ¨‹å£ä¸€è‘‰', 'romance'),
    '1206': ('å©¦ç³»å›³', 'æ³‰é¡èŠ±', 'romance'),
    
    # scifi
    '46897': ('åœ°çƒç›—é›£', 'æµ·é‡åä¸‰', 'scifi'),
    '456': ('éŠ€æ²³é‰„é“ã®å¤œ', 'å®®æ²¢è³¢æ²»', 'scifi'),
    '461': ('é¢¨ã®åˆä¸‰éƒ', 'å®®æ²¢è³¢æ²»', 'scifi'),
    
    # comedy
    '275': ('èµ°ã‚Œãƒ¡ãƒ­ã‚¹', 'å¤ªå®°æ²»', 'comedy'),
    '312': ('å±±æ¤’é­š', 'äº•ä¼é°’äºŒ', 'comedy'),
    
    # neutral
    '789': ('å¾Œå°‘å¹´', 'å¤ç›®æ¼±çŸ³', 'neutral'),
    '773': ('å¾è¼©ã¯çŒ«ã§ã‚ã‚‹', 'å¤ç›®æ¼±çŸ³', 'neutral'),
    '879': ('ç¾…ç”Ÿé–€', 'èŠ¥å·é¾ä¹‹ä»‹', 'neutral'),
    '74': ('èœœè››ã®ç³¸', 'èŠ¥å·é¾ä¹‹ä»‹', 'neutral'),
}

def download_aozora_text(work_id: str) -> str:
    """é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    # é’ç©ºæ–‡åº«ã®GitHubã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    base_url = "https://www.aozora.gr.jp/cards/{author_id}/files/{work_id}_ruby.zip"
    
    # ä½œå“IDã‹ã‚‰ä½œè€…IDã‚’æ¨å®šï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚å›ºå®šãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
    author_map = {
        '482': '001779', '427': '001779',  # æ±Ÿæˆ¸å·ä¹±æ­©
        '235': '000096', '4308': '000096',  # å¤¢é‡ä¹…ä½œ
        '756': '000885',  # ä¸è¬é‡æ™¶å­
        '1569': '000064', '1147': '000064',  # æ¨‹å£ä¸€è‘‰
        '1206': '001029',  # æ³‰é¡èŠ±
        '46897': '000160',  # æµ·é‡åä¸‰
        '456': '000081', '461': '000081',  # å®®æ²¢è³¢æ²»
        '275': '000035',  # å¤ªå®°æ²»
        '312': '000058',  # äº•ä¼é°’äºŒ
        '789': '000148', '773': '000148',  # å¤ç›®æ¼±çŸ³
        '879': '000879', '74': '000879',  # èŠ¥å·é¾ä¹‹ä»‹
    }
    
    author_id = author_map.get(work_id)
    if not author_id:
        print(f"âš ï¸ Work ID {work_id} not mapped to author")
        return ""
    
    url = base_url.format(author_id=author_id, work_id=work_id)
    
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            for filename in z.namelist():
                if filename.endswith('.txt'):
                    with z.open(filename) as f:
                        # Shift-JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰
                        content = f.read().decode('shift-jis', errors='ignore')
                        return content
    except Exception as e:
        print(f"âŒ Failed to download work {work_id}: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å–å¾—ã‚’è©¦ã¿ã‚‹
        try:
            txt_url = f"https://www.aozora.gr.jp/cards/{author_id}/files/{work_id}.txt"
            response = requests.get(txt_url, timeout=30)
            response.encoding = 'shift-jis'
            return response.text
        except:
            return ""
    
    return ""

def preprocess_aozora(raw_text: str) -> str:
    """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’é™¤å»ï¼ˆ-------ä»¥é™ãŒæœ¬æ–‡ï¼‰
    if '-------' in raw_text:
        parts = raw_text.split('-------')
        if len(parts) >= 2:
            raw_text = parts[-1]
    
    # ãƒ«ãƒ“ã‚’é™¤å»: ï½œæ¼¢å­—ã€Šã‹ã‚“ã˜ã€‹ â†’ æ¼¢å­—
    text = re.sub(r'ï½œ([^ã€Š]+)ã€Š[^ã€‹]+ã€‹', r'\1', raw_text)
    text = re.sub(r'([^ï½œ])ã€Š[^ã€‹]+ã€‹', r'\1', text)
    
    # æ³¨è¨˜ã‚’é™¤å»: ï¼»ï¼ƒ...ï¼½
    text = re.sub(r'ï¼»ï¼ƒ[^ï¼½]+ï¼½', '', text)
    
    # ç©ºç™½è¡Œã‚’æ•´ç†
    text = re.sub(r'\n\s*\n+', '\n', text)
    
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()
    
    return text

def extract_words_by_genre(text: str, genre: str) -> Dict[str, List[str]]:
    """ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã«å˜èªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    # MeCabã‚’ä½¿ã‚ãšã«æ­£è¦è¡¨ç¾ã§ç°¡æ˜“æŠ½å‡º
    slots = {
        'ä¸»ä½“': [],
        'å ´æ‰€': [],
        'ç™ºè¦‹ç‰©': [],
        'å‹•ä½œ': [],
        'æ„Ÿæƒ…': [],
    }
    
    # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = {
        'horror': {
            'ä¸»ä½“': ['å¹½éœŠ', 'å½±', 'ä½•è€…', 'æ€ªç‰©', 'é»’ã„', 'æ­»è€…', 'äº¡éœŠ', 'æ‚ªé­”'],
            'å ´æ‰€': ['å¢“', 'å¢“åœ°', 'å¢“å ´', 'å»ƒå±‹', 'æš—é—‡', 'åœ°ä¸‹', 'æ´çªŸ', 'æ£®'],
            'ç™ºè¦‹ç‰©': ['è¡€', 'ãƒŠã‚¤ãƒ•', 'åˆ€', 'éª¸', 'æ­»ä½“', 'æ—¥è¨˜', 'æ‰‹ç´™'],
            'å‹•ä½œ': ['éœ‡ãˆ', 'å«ã³', 'å«ã‚“', 'é€ƒã’', 'è¥²ã„', 'ã†ã‚ã', 'æ€–'],
            'æ„Ÿæƒ…': ['ææ€–', 'æã‚ã—', 'ä¸å®‰', 'ä¸æ°—å‘³', 'æ€–ã„', 'æ€¯ãˆ', 'ææ€–']
        },
        'romance': {
            'ä¸»ä½“': ['å›', 'ã‚ãªãŸ', 'æ‹äºº', 'å½¼', 'å½¼å¥³', 'äºŒäºº', 'ç§ãŸã¡'],
            'å ´æ‰€': ['å…¬åœ’', 'ã‚«ãƒ•ã‚§', 'æµ·', 'æ©‹', 'é§…', 'ãƒ™ãƒ³ãƒ', 'æ•™ä¼š'],
            'ç™ºè¦‹ç‰©': ['èŠ±', 'æ‰‹ç´™', 'æŒ‡è¼ª', 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ', 'å†™çœŸ', 'æ—¥è¨˜'],
            'å‹•ä½œ': ['æŠ±ã', 'ã‚­ã‚¹', 'æ‰‹ã‚’', 'å¾®ç¬‘', 'è¦‹ã¤ã‚', 'ç¬‘ã£'],
            'æ„Ÿæƒ…': ['æ„›', 'æ‹', 'å¹¸ã›', 'å„ªã—', 'åˆ‡ãª', 'æ‚²ã—', 'å¬‰ã—']
        },
        'scifi': {
            'ä¸»ä½“': ['ãƒ­ãƒœãƒƒãƒˆ', 'æ©Ÿæ¢°', 'å®‡å®™', 'ç§‘å­¦è€…', 'ç ”ç©¶è€…', 'åšå£«'],
            'å ´æ‰€': ['å®‡å®™', 'ç ”ç©¶æ‰€', 'å®Ÿé¨“å®¤', 'ç«æ˜Ÿ', 'æœˆ', 'åŸºåœ°', 'èˆ¹'],
            'ç™ºè¦‹ç‰©': ['æ©Ÿæ¢°', 'è£…ç½®', 'ãƒ‡ãƒ¼ã‚¿', 'é›»æ³¢', 'ä¿¡å·', 'ã‚¨ãƒãƒ«ã‚®ãƒ¼'],
            'å‹•ä½œ': ['åˆ†æ', 'è¨ˆç®—', 'è¦³æ¸¬', 'å®Ÿé¨“', 'ç™ºå°„', 'æ¢æŸ»'],
            'æ„Ÿæƒ…': ['é©šã', 'ç™ºè¦‹', 'é€²æ­©', 'æœªæ¥', 'ç§‘å­¦çš„']
        },
        'comedy': {
            'ä¸»ä½“': ['å¤‰ãª', 'ãŠã‹ã—ãª', 'ãƒ‰ã‚¸', 'ãƒãƒŒã‚±', 'ãŠã£ã¡ã‚‡ã“ã¡ã‚‡ã„'],
            'å ´æ‰€': ['èˆå°', 'ã‚µãƒ¼ã‚«ã‚¹', 'ç¥­', 'åºƒå ´', 'å¸‚å ´'],
            'ç™ºè¦‹ç‰©': ['ãƒãƒŠãƒŠ', 'ãƒ‘ã‚¤', 'å¤‰ãª', 'ãŠã‹ã—ãª'],
            'å‹•ä½œ': ['è»¢ã³', 'è»¢ã‚“', 'æ»‘ã£', 'ã¶ã¤ã‹', 'å€’ã‚Œ'],
            'æ„Ÿæƒ…': ['æ¥½ã—', 'ãŠã‹ã—', 'ç¬‘', 'é¢ç™½', 'æ„‰å¿«']
        },
        'neutral': {
            'ä¸»ä½“': ['äºº', 'å½¼', 'å½¼å¥³', 'ç§', 'ã¿ã‚“ãª', 'å…ˆç”Ÿ', 'å­ä¾›'],
            'å ´æ‰€': ['éƒ¨å±‹', 'å®¶', 'å­¦æ ¡', 'å…¬åœ’', 'è¡—', 'é§…', 'åº—'],
            'ç™ºè¦‹ç‰©': ['æœ¬', 'æ‰‹ç´™', 'æ–°è', 'æ™‚è¨ˆ', 'ã‚«ãƒãƒ³', 'å†™çœŸ'],
            'å‹•ä½œ': ['æ­©ã', 'è©±ã™', 'è¦‹ã‚‹', 'è€ƒãˆã‚‹', 'èª­ã‚€', 'æ›¸ã'],
            'æ„Ÿæƒ…': ['å¬‰ã—', 'æ‚²ã—', 'é©š', 'ä¸æ€è­°', 'ç©ã‚„ã‹']
        }
    }
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    genre_patterns = patterns.get(genre, patterns['neutral'])
    
    for slot_type, keywords in genre_patterns.items():
        for keyword in keywords:
            count = text.count(keyword)
            if count > 0:
                slots[slot_type].extend([keyword] * min(count, 3))  # æœ€å¤§ï¼“å›ã¾ã§
    
    return slots

def extract_sentence_patterns(text: str, genre: str) -> List[str]:
    """æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º"""
    templates = []
    
    # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã®æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³
    if genre == 'horror':
        patterns = [
            r'(.+)ãŒ(.+)ã§(.+)ã‚’è¦‹ã¤ã‘ãŸ',
            r'(.+)ã¯ææ€–ã«(.+)',
            r'(.+)ã‹ã‚‰(.+)ãŒ(.+)',
            r'æš—é—‡ã®ä¸­(.+)',
        ]
    elif genre == 'romance':
        patterns = [
            r'(.+)ã¯(.+)ã‚’æ„›ã—ã¦',
            r'(.+)ãŒ(.+)ã«å¾®ç¬‘ã‚“',
            r'(.+)ã¨(.+)ã¯',
            r'äºŒäººã¯(.+)',
        ]
    elif genre == 'scifi':
        patterns = [
            r'(.+)ã‚’åˆ†æ(.+)',
            r'(.+)ã®ãƒ‡ãƒ¼ã‚¿(.+)',
            r'ã‚·ã‚¹ãƒ†ãƒ ãŒ(.+)',
            r'(.+)ã‚’è¦³æ¸¬(.+)',
        ]
    else:
        patterns = [
            r'(.+)ã¯(.+)ã§ã‚ã‚‹',
            r'(.+)ãŒ(.+)ã—ãŸ',
            r'(.+)ã‚’(.+)ã—ã¦',
        ]
    
    # æ–‡ã‚’æŠ½å‡º
    sentences = text.split('ã€‚')
    for sentence in sentences[:100]:  # æœ€åˆã®100æ–‡ã®ã¿
        for pattern in patterns:
            if re.search(pattern, sentence):
                # ã‚¹ãƒ­ãƒƒãƒˆåŒ–
                template = re.sub(pattern, r'{ä¸»ä½“}ãŒ{å ´æ‰€}ã§{ç™ºè¦‹ç‰©}ã‚’è¦‹ã¤ã‘ãŸ', sentence)
                if template not in templates:
                    templates.append(template)
                    if len(templates) >= 5:  # ã‚¸ãƒ£ãƒ³ãƒ«ã”ã¨ï¼•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¾ã§
                        break
    
    return templates

def extract_phrases(text: str, genre: str) -> List[str]:
    """ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
    phrases = []
    
    # ã‚¸ãƒ£ãƒ³ãƒ«ç‰¹æœ‰ã®ãƒ•ãƒ¬ãƒ¼ã‚º
    if genre == 'horror':
        phrase_patterns = ['èƒŒç­‹ãŒå‡', 'è¡€ã®æ°—ãŒ', 'é—‡ã®ä¸­', 'æ€–ã„ã»ã©', 'ä¸æ°—å‘³ãª']
    elif genre == 'romance':
        phrase_patterns = ['æ„›ã—ã¦', 'å¿ƒãŒ', 'äºŒäººã®', 'æ°¸é ã«', 'å„ªã—ã']
    elif genre == 'scifi':
        phrase_patterns = ['ãƒ‡ãƒ¼ã‚¿ã«', 'ã‚·ã‚¹ãƒ†ãƒ ', 'åˆ†æ', 'æœªæ¥', 'ç§‘å­¦']
    elif genre == 'comedy':
        phrase_patterns = ['ãƒ‰ã‚¿ãƒã‚¿', 'ã†ã£ã‹ã‚Š', 'ãŸã¾ãŸã¾', 'ãŠã£ã¡ã‚‡ã“ã¡ã‚‡ã„']
    else:
        phrase_patterns = ['ãã‚Œã¯', 'ã—ã‹ã—', 'ãã—ã¦', 'ã ãŒ', 'ã‚„ãŒã¦']
    
    for pattern in phrase_patterns:
        if pattern in text:
            phrases.append(pattern)
    
    return phrases

def process_and_insert_to_db(conn_info: dict):
    """é’ç©ºæ–‡åº«ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦DBã«æŠ•å…¥"""
    # PostgreSQLæ¥ç¶š
    conn = psycopg2.connect(
        host=conn_info['endpoint'],
        port=conn_info['port'],
        database=conn_info['database'],
        user=conn_info['username'],
        password=conn_info['password']
    )
    cur = conn.cursor()
    
    print("ğŸ“š é’ç©ºæ–‡åº«ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
    print("=" * 60)
    
    # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ã«é›†è¨ˆ
    genre_data = defaultdict(lambda: {
        'words': defaultdict(list),
        'templates': [],
        'phrases': []
    })
    
    # å„ä½œå“ã‚’å‡¦ç†
    for work_id, (title, author, genre) in AOZORA_WORKS.items():
        print(f"\nğŸ“– Processing: {title} by {author} ({genre})...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        text = download_aozora_text(work_id)
        if not text:
            print(f"  âš ï¸ Skipped (download failed)")
            continue
        
        # å‰å‡¦ç†
        text = preprocess_aozora(text)
        print(f"  âœ… Text length: {len(text)} chars")
        
        # å˜èªæŠ½å‡º
        words = extract_words_by_genre(text, genre)
        for slot_type, word_list in words.items():
            genre_data[genre]['words'][slot_type].extend(word_list)
        
        # æ–‡ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæŠ½å‡º
        templates = extract_sentence_patterns(text, genre)
        genre_data[genre]['templates'].extend(templates)
        
        # ãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡º
        phrases = extract_phrases(text, genre)
        genre_data[genre]['phrases'].extend(phrases)
    
    print("\n" + "=" * 60)
    print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŠ•å…¥ä¸­...")
    
    # DBã«æŠ•å…¥
    for genre, data in genre_data.items():
        print(f"\nğŸ¨ Genre: {genre}")
        
        # 1. corpus_wordsã«æŠ•å…¥
        for slot_type, word_list in data['words'].items():
            if word_list:
                word_freq = Counter(word_list)
                for word, count in word_freq.most_common(20):  # å„ã‚¹ãƒ­ãƒƒãƒˆä¸Šä½20èª
                    weight = min(1.0, count / 10.0)
                    cur.execute("""
                        INSERT INTO corpus_words (id, genre, slot_type, word, weight)
                        VALUES (%s, %s, %s, %s, %s)
                        ON CONFLICT (genre, slot_type, word) 
                        DO UPDATE SET weight = GREATEST(corpus_words.weight, EXCLUDED.weight)
                    """, (str(uuid.uuid4()), genre, slot_type, word, weight))
        
        print(f"  âœ… Words inserted")
        
        # 2. sentence_templatesã«æŠ•å…¥
        unique_templates = list(set(data['templates']))[:5]  # é‡è¤‡é™¤å»ã—ã¦ä¸Šä½5å€‹
        for template in unique_templates:
            if template:
                cur.execute("""
                    INSERT INTO sentence_templates (id, template_type, template, genre)
                    VALUES (%s, %s, %s, %s)
                    ON CONFLICT (template_type, template) DO NOTHING
                """, (str(uuid.uuid4()), 'auto_extracted', template, genre))
        
        print(f"  âœ… Templates inserted")
        
        # 3. phrase_patternsã«æŠ•å…¥
        unique_phrases = list(set(data['phrases']))[:10]  # é‡è¤‡é™¤å»ã—ã¦ä¸Šä½10å€‹
        for phrase in unique_phrases:
            if phrase:
                cur.execute("""
                    INSERT INTO phrase_patterns (id, genre, phrase)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (genre, phrase) DO NOTHING
                """, (str(uuid.uuid4()), genre, phrase))
        
        print(f"  âœ… Phrases inserted")
    
    # ã‚³ãƒŸãƒƒãƒˆ
    conn.commit()
    
    # çµ±è¨ˆè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š æŠ•å…¥çµæœ:")
    
    cur.execute("SELECT genre, COUNT(*) FROM corpus_words GROUP BY genre")
    for row in cur.fetchall():
        print(f"  corpus_words ({row[0]}): {row[1]} entries")
    
    cur.execute("SELECT genre, COUNT(*) FROM sentence_templates WHERE genre IS NOT NULL GROUP BY genre")
    for row in cur.fetchall():
        print(f"  templates ({row[0]}): {row[1]} entries")
    
    cur.execute("SELECT genre, COUNT(*) FROM phrase_patterns GROUP BY genre")
    for row in cur.fetchall():
        print(f"  phrases ({row[0]}): {row[1]} entries")
    
    print("\nâœ… é’ç©ºæ–‡åº«ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥å®Œäº†ï¼")
    
    cur.close()
    conn.close()

if __name__ == "__main__":
    # æ¥ç¶šæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    with open('rds_connection_info.json', 'r') as f:
        conn_info = json.load(f)
    
    # å‡¦ç†å®Ÿè¡Œ
    process_and_insert_to_db(conn_info)